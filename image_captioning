{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, Dropout, Flatten, Dense, Input, Layer\nfrom tensorflow.keras.layers import Embedding, LSTM, add, Concatenate, Reshape, concatenate, Bidirectional\nfrom tensorflow.keras.applications import VGG16, ResNet50, DenseNet201\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom textwrap import wrap\n\nsns.set_style(\"dark\")\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-01T10:21:05.168757Z","iopub.execute_input":"2024-05-01T10:21:05.169138Z","iopub.status.idle":"2024-05-01T10:21:25.371401Z","shell.execute_reply.started":"2024-05-01T10:21:05.169108Z","shell.execute_reply":"2024-05-01T10:21:25.370418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_path = '../input/flickr8k/Images'\ndata = pd.read_csv(\"../input/flickr8k/captions.txt\")\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T10:21:25.373433Z","iopub.execute_input":"2024-05-01T10:21:25.374411Z","iopub.status.idle":"2024-05-01T10:21:25.509059Z","shell.execute_reply.started":"2024-05-01T10:21:25.374378Z","shell.execute_reply":"2024-05-01T10:21:25.508130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def readImage(path,img_size=224):\n    img = load_img(path,color_mode='rgb',target_size=(img_size,img_size))\n    img = img_to_array(img)\n    img = img/255.\n    \n    return img\n\ndef display_images(temp_df):\n    temp_df = temp_df.reset_index(drop=True)\n    plt.figure(figsize = (20 , 20))\n    n = 0\n    for i in range(15):\n        n+=1\n        plt.subplot(5 , 5, n)\n        plt.subplots_adjust(hspace = 0.7, wspace = 0.3)\n        image = readImage(f\"../input/flickr8k/Images/{temp_df.image[i]}\")\n        plt.imshow(image)\n        plt.title(\"\\n\".join(wrap(temp_df.caption[i], 20)))\n        plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2024-05-01T10:21:25.510278Z","iopub.execute_input":"2024-05-01T10:21:25.510609Z","iopub.status.idle":"2024-05-01T10:21:25.517760Z","shell.execute_reply.started":"2024-05-01T10:21:25.510582Z","shell.execute_reply":"2024-05-01T10:21:25.516927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_images(data.sample(15))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T10:21:25.519628Z","iopub.execute_input":"2024-05-01T10:21:25.519923Z","iopub.status.idle":"2024-05-01T10:21:28.800827Z","shell.execute_reply.started":"2024-05-01T10:21:25.519900Z","shell.execute_reply":"2024-05-01T10:21:28.799774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_preprocessing(data):\n    data['caption'] = data['caption'].apply(lambda x: x.lower())\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\",\"\"))\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\",\" \"))\n    data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word)>1]))\n    data['caption'] = \"start \"+data['caption']+\" end\"\n    return data","metadata":{"execution":{"iopub.status.busy":"2024-05-01T10:21:28.802050Z","iopub.execute_input":"2024-05-01T10:21:28.802365Z","iopub.status.idle":"2024-05-01T10:21:28.812114Z","shell.execute_reply.started":"2024-05-01T10:21:28.802337Z","shell.execute_reply":"2024-05-01T10:21:28.808753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = text_preprocessing(data)\ncaptions = data['caption'].tolist()\ncaptions[:10]","metadata":{"execution":{"iopub.status.busy":"2024-05-01T10:21:28.813296Z","iopub.execute_input":"2024-05-01T10:21:28.813682Z","iopub.status.idle":"2024-05-01T10:21:29.089002Z","shell.execute_reply.started":"2024-05-01T10:21:28.813621Z","shell.execute_reply":"2024-05-01T10:21:29.088076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(captions)\nvocab_size = len(tokenizer.word_index) + 1\nmax_length = max(len(caption.split()) for caption in captions)\n\nimages = data['image'].unique().tolist()\nnimages = len(images)\n\nsplit_index = round(0.75*nimages)\ntrain_images = images[:split_index]\nval_images = images[split_index:]\n\ntrain = data[data['image'].isin(train_images)]\ntest = data[data['image'].isin(val_images)]\n\ntrain.reset_index(inplace=True,drop=True)\ntest.reset_index(inplace=True,drop=True)\n\ntokenizer.texts_to_sequences([captions[1]])[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-01T10:21:29.090189Z","iopub.execute_input":"2024-05-01T10:21:29.090468Z","iopub.status.idle":"2024-05-01T10:21:29.874729Z","shell.execute_reply.started":"2024-05-01T10:21:29.090444Z","shell.execute_reply":"2024-05-01T10:21:29.873845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DenseNet201()\nfe = Model(inputs=model.input, outputs=model.layers[-2].output)\n\nimg_size = 224\nfeatures = {}\nfor image in tqdm(data['image'].unique().tolist()):\n    img = load_img(os.path.join(image_path,image),target_size=(img_size,img_size))\n    img = img_to_array(img)\n    img = img/255.\n    img = np.expand_dims(img,axis=0)\n    feature = fe.predict(img, verbose=0)\n    features[image] = feature","metadata":{"execution":{"iopub.status.busy":"2024-05-01T10:21:29.875846Z","iopub.execute_input":"2024-05-01T10:21:29.876141Z","iopub.status.idle":"2024-05-01T10:34:39.184036Z","shell.execute_reply.started":"2024-05-01T10:21:29.876116Z","shell.execute_reply":"2024-05-01T10:34:39.182923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataGenerator(Sequence):\n    \n    def __init__(self, df, X_col, y_col, batch_size, directory, tokenizer, \n                 vocab_size, max_length, features,shuffle=True):\n    \n        self.df = df.copy()\n        self.X_col = X_col\n        self.y_col = y_col\n        self.directory = directory\n        self.batch_size = batch_size\n        self.tokenizer = tokenizer\n        self.vocab_size = vocab_size\n        self.max_length = max_length\n        self.features = features\n        self.shuffle = shuffle\n        self.n = len(self.df)\n        \n    def on_epoch_end(self):\n        if self.shuffle:\n            self.df = self.df.sample(frac=1).reset_index(drop=True)\n    \n    def __len__(self):\n        return self.n // self.batch_size\n    \n    def __getitem__(self,index):\n    \n        batch = self.df.iloc[index * self.batch_size:(index + 1) * self.batch_size,:]\n        X1, X2, y = self.__get_data(batch)        \n        return (X1, X2), y\n    def __get_data(self,batch):\n        \n        X1, X2, y = list(), list(), list()\n        \n        images = batch[self.X_col].tolist()\n           \n        for image in images:\n            feature = self.features[image][0]\n            \n            captions = batch.loc[batch[self.X_col]==image, self.y_col].tolist()\n            for caption in captions:\n                seq = self.tokenizer.texts_to_sequences([caption])[0]\n\n                for i in range(1,len(seq)):\n                    in_seq, out_seq = seq[:i], seq[i]\n                    in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]\n                    out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]\n                    X1.append(feature)\n                    X2.append(in_seq)\n                    y.append(out_seq)\n            \n        X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n                \n        return X1, X2, y","metadata":{"execution":{"iopub.status.busy":"2024-05-01T10:34:39.185423Z","iopub.execute_input":"2024-05-01T10:34:39.185781Z","iopub.status.idle":"2024-05-01T10:34:39.202308Z","shell.execute_reply.started":"2024-05-01T10:34:39.185751Z","shell.execute_reply":"2024-05-01T10:34:39.201310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input1 = Input(shape=(1920,))\ninput2 = Input(shape=(max_length,))\n\nimg_features = Dense(256, activation='relu')(input1)\nimg_features_reshaped = Reshape((1, 256), input_shape=(256,))(img_features)\n\nsentence_features = Embedding(vocab_size, 256, mask_zero=False)(input2)\nmerged = concatenate([img_features_reshaped,sentence_features],axis=1)\nsentence_features = LSTM(256)(merged)\nx = Dropout(0.5)(sentence_features)\nx = add([x, img_features])\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.5)(x)\noutput = Dense(vocab_size, activation='softmax')(x)\n\ncaption_model = Model(inputs=[input1,input2], outputs=output)\ncaption_model.compile(loss='categorical_crossentropy',optimizer='adam')","metadata":{"execution":{"iopub.status.busy":"2024-05-01T10:34:39.205800Z","iopub.execute_input":"2024-05-01T10:34:39.206107Z","iopub.status.idle":"2024-05-01T10:34:39.560484Z","shell.execute_reply.started":"2024-05-01T10:34:39.206080Z","shell.execute_reply":"2024-05-01T10:34:39.559689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\nplot_model(caption_model)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T10:34:39.561618Z","iopub.execute_input":"2024-05-01T10:34:39.561973Z","iopub.status.idle":"2024-05-01T10:34:40.168559Z","shell.execute_reply.started":"2024-05-01T10:34:39.561937Z","shell.execute_reply":"2024-05-01T10:34:40.167625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"caption_model.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T10:34:40.169624Z","iopub.execute_input":"2024-05-01T10:34:40.169920Z","iopub.status.idle":"2024-05-01T10:34:40.200787Z","shell.execute_reply.started":"2024-05-01T10:34:40.169896Z","shell.execute_reply":"2024-05-01T10:34:40.199940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_generator = CustomDataGenerator(df=train,X_col='image',y_col='caption',batch_size=64,directory=image_path,\n                                      tokenizer=tokenizer,vocab_size=vocab_size,max_length=max_length,features=features)\n\nvalidation_generator = CustomDataGenerator(df=test,X_col='image',y_col='caption',batch_size=64,directory=image_path,\n                                      tokenizer=tokenizer,vocab_size=vocab_size,max_length=max_length,features=features)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T10:34:40.201947Z","iopub.execute_input":"2024-05-01T10:34:40.202341Z","iopub.status.idle":"2024-05-01T10:34:40.221595Z","shell.execute_reply.started":"2024-05-01T10:34:40.202311Z","shell.execute_reply":"2024-05-01T10:34:40.220936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"model.keras\"\ncheckpoint = ModelCheckpoint(model_name,\n                            monitor=\"val_loss\",\n                            mode=\"min\",\n                            save_best_only = True,\n                            verbose=1)\n\nearlystopping = EarlyStopping(monitor='val_loss',min_delta = 0, patience = 5, verbose = 1, restore_best_weights=True)\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.2, \n                                            min_lr=0.00000001)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T10:34:40.222494Z","iopub.execute_input":"2024-05-01T10:34:40.222751Z","iopub.status.idle":"2024-05-01T10:34:40.228615Z","shell.execute_reply.started":"2024-05-01T10:34:40.222728Z","shell.execute_reply":"2024-05-01T10:34:40.227680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = caption_model.fit(\n        train_generator,\n        epochs=10,\n        validation_data=validation_generator,\n        callbacks=[checkpoint,earlystopping,learning_rate_reduction])","metadata":{"execution":{"iopub.status.busy":"2024-05-01T10:34:40.229758Z","iopub.execute_input":"2024-05-01T10:34:40.230025Z","iopub.status.idle":"2024-05-01T10:52:46.490102Z","shell.execute_reply.started":"2024-05-01T10:34:40.230001Z","shell.execute_reply":"2024-05-01T10:52:46.489331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20,8))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T10:52:46.491424Z","iopub.execute_input":"2024-05-01T10:52:46.491753Z","iopub.status.idle":"2024-05-01T10:52:46.908170Z","shell.execute_reply.started":"2024-05-01T10:52:46.491728Z","shell.execute_reply":"2024-05-01T10:52:46.907173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def idx_to_word(integer,tokenizer):\n    \n    for word, index in tokenizer.word_index.items():\n        if index==integer:\n            return word\n    return None","metadata":{"execution":{"iopub.status.busy":"2024-05-01T10:52:46.909696Z","iopub.execute_input":"2024-05-01T10:52:46.910429Z","iopub.status.idle":"2024-05-01T10:52:46.915285Z","shell.execute_reply.started":"2024-05-01T10:52:46.910391Z","shell.execute_reply":"2024-05-01T10:52:46.914414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_caption(model, image, tokenizer, max_length, features):\n    \n    feature = features[image]\n    in_text = \"start\"\n    for i in range(max_length):\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        sequence = pad_sequences([sequence], max_length)\n\n        y_pred = model.predict([feature,sequence])\n        y_pred = np.argmax(y_pred)\n        \n        word = idx_to_word(y_pred, tokenizer)\n        \n        if word is None:\n            break\n            \n        in_text+= \" \" + word\n        \n        if word == 'end':\n            break\n            \n    return in_text ","metadata":{"execution":{"iopub.status.busy":"2024-05-01T10:52:46.916413Z","iopub.execute_input":"2024-05-01T10:52:46.916727Z","iopub.status.idle":"2024-05-01T10:52:46.929779Z","shell.execute_reply.started":"2024-05-01T10:52:46.916703Z","shell.execute_reply":"2024-05-01T10:52:46.928910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"samples = test.sample(15)\nsamples.reset_index(drop=True,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T10:52:46.930883Z","iopub.execute_input":"2024-05-01T10:52:46.931599Z","iopub.status.idle":"2024-05-01T10:52:46.941929Z","shell.execute_reply.started":"2024-05-01T10:52:46.931574Z","shell.execute_reply":"2024-05-01T10:52:46.941002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index,record in samples.iterrows():\n\n    img = load_img(os.path.join(image_path,record['image']),target_size=(224,224))\n    img = img_to_array(img)\n    img = img/255.\n    \n    caption = predict_caption(caption_model, record['image'], tokenizer, max_length, features)\n    samples.loc[index,'caption'] = caption","metadata":{"execution":{"iopub.status.busy":"2024-05-01T10:52:46.942875Z","iopub.execute_input":"2024-05-01T10:52:46.943144Z","iopub.status.idle":"2024-05-01T10:52:55.129361Z","shell.execute_reply.started":"2024-05-01T10:52:46.943115Z","shell.execute_reply":"2024-05-01T10:52:55.128578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_images(samples)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T10:52:55.130448Z","iopub.execute_input":"2024-05-01T10:52:55.130782Z","iopub.status.idle":"2024-05-01T10:52:58.086822Z","shell.execute_reply.started":"2024-05-01T10:52:55.130754Z","shell.execute_reply":"2024-05-01T10:52:58.085680Z"},"trusted":true},"execution_count":null,"outputs":[]}]}